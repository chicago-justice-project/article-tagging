{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin.rose\\Documents\\GitHub\\cjp-article-tagging\\lib\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tagnews\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n",
      "C:\\Users\\kevin.rose\\AppData\\Local\\Continuum\\Anaconda2\\envs\\cjp\\lib\\site-packages\\numpy\\lib\\arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "glove = tagnews.load_glove('tagnews/data/glove.6B.50d.txt')\n",
    "ner = tagnews.load_ner_data('tagnews/data/')\n",
    "ner = pd.concat([ner, pd.DataFrame(glove.loc[ner['word'].str.lower()].values)], axis='columns')\n",
    "ner.fillna(value=0.0, inplace=True)\n",
    "\n",
    "data_dim = 50\n",
    "timesteps = 25 # only during training, testing can take arbitrary length.\n",
    "num_classes = 2\n",
    "\n",
    "train_val_split = int(19 * ner.shape[0] / 20.)\n",
    "\n",
    "ner_train_idxs = range(0, train_val_split - timesteps, timesteps)\n",
    "x_train = np.array([ner.iloc[i:i+timesteps, 3:].values\n",
    "                    for i in ner_train_idxs])\n",
    "y_train = np.array([to_categorical(ner.iloc[i:i+timesteps, 2].values, 2)\n",
    "                    for i in ner_train_idxs])\n",
    "\n",
    "ner_val_idxs = range(train_val_split, ner.shape[0] - timesteps, timesteps)\n",
    "x_val = np.array([ner.iloc[i:i+timesteps, 3:].values\n",
    "                  for i in ner_val_idxs])\n",
    "y_val = np.array([to_categorical(ner.iloc[i:i+timesteps, 2].values, 2)\n",
    "                  for i in ner_val_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                                 Output Shape                            Param #        \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                                (None, None, 32)                        10624          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                                (None, None, 8)                         1312           \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistributed)         (None, None, 2)                         18             \n",
      "====================================================================================================\n",
      "Total params: 11,954\n",
      "Trainable params: 11,954\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(None, data_dim)))\n",
    "model.add(LSTM(8, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(2, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(model.summary(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='./tmp/weights-{epoch:02d}.hdf5',\n",
    "                               monitor='val_categorical_accuracy',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "class OurAUC(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Go to https://geo-extract-tester.herokuapp.com/ and download\n",
    "        # the validation data (validation.txt).\n",
    "        with open('validation.txt', encoding='utf-8') as f:\n",
    "            s = f.read()\n",
    "\n",
    "        gloved_data = glove.loc[[w for w in s.split('\\n') if w]].fillna(0)\n",
    "        glove_time_size=100\n",
    "        preds_batched = []\n",
    "        i = 0\n",
    "        while gloved_data[i:i+glove_time_size].size:\n",
    "            preds_batched.append(model.predict(np.expand_dims(gloved_data[i:i+glove_time_size],\n",
    "                                                              axis=0))[0][:,1])\n",
    "            i += glove_time_size\n",
    "\n",
    "        with open('guesses-{epoch:02d}.txt'.format(epoch=epoch), 'w') as f:\n",
    "            for prob in [p for pred in preds_batched for p in pred]:\n",
    "                f.write(str(prob) + '\\n')\n",
    "\n",
    "        with open('guesses-{epoch:02d}.txt'.format(epoch=epoch), 'rb') as f:\n",
    "            url = 'https://geo-extract-tester.herokuapp.com/api/score'\n",
    "            r = requests.post(url, files={'file': f})\n",
    "            print('AUC: {:.5f}'.format(json.loads(r.text)['auc']))\n",
    "\n",
    "our_auc = OurAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39930 samples, validate on 2101 samples\n",
      "Epoch 1/5\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.98226, saving model to ./tmp/weights-01.hdf5\n",
      "AUC: 0.84700\n",
      " - 70s - loss: 0.0988 - categorical_accuracy: 0.9680 - val_loss: 0.0528 - val_categorical_accuracy: 0.9823\n",
      "Epoch 2/5\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.98226 to 0.98359, saving model to ./tmp/weights-02.hdf5\n",
      "AUC: 0.84535\n",
      " - 49s - loss: 0.0463 - categorical_accuracy: 0.9837 - val_loss: 0.0451 - val_categorical_accuracy: 0.9836\n",
      "Epoch 3/5\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.98359 to 0.98462, saving model to ./tmp/weights-03.hdf5\n",
      "AUC: 0.85409\n",
      " - 53s - loss: 0.0414 - categorical_accuracy: 0.9850 - val_loss: 0.0417 - val_categorical_accuracy: 0.9846\n",
      "Epoch 4/5\n",
      "Epoch 00004: val_categorical_accuracy did not improve\n",
      "AUC: 0.85198\n",
      " - 57s - loss: 0.0390 - categorical_accuracy: 0.9855 - val_loss: 0.0405 - val_categorical_accuracy: 0.9846\n",
      "Epoch 5/5\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.98462 to 0.98515, saving model to ./tmp/weights-05.hdf5\n",
      "AUC: 0.85425\n",
      " - 52s - loss: 0.0374 - categorical_accuracy: 0.9861 - val_loss: 0.0396 - val_categorical_accuracy: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1735b630eb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[checkpointer, our_auc],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word all_tags    tag  prob_geloc\n",
      "0             said        O  False    0.000316\n",
      "1               it        O  False    0.000067\n",
      "2             will        O  False    0.000115\n",
      "3               go        O  False    0.000067\n",
      "4            ahead        O  False    0.000316\n",
      "5             with        O  False    0.000034\n",
      "6                a        O  False    0.000028\n",
      "7   reconciliation        O  False    0.000495\n",
      "8       conference        O  False    0.000674\n",
      "9               to        O  False    0.000034\n",
      "10           which        O  False    0.001015\n",
      "11            more        O  False    0.000032\n",
      "12            than        O  False    0.000035\n",
      "13           1,300        O  False    0.000469\n",
      "14          Somali    B-gpe  False    0.002423\n",
      "15          elders        O  False    0.000335\n",
      "16               ,        O  False    0.000028\n",
      "17        warlords        O  False    0.000712\n",
      "18             and        O  False    0.000040\n",
      "19     politicians        O  False    0.000133\n",
      "20             are        O  False    0.000022\n",
      "21         invited        O  False    0.000244\n",
      "22               .        O  False    0.000034\n",
      "23           Iraqi    B-gpe  False    0.001136\n",
      "24        military        O  False    0.000084\n",
      "25       officials        O  False    0.000022\n",
      "26             say        O  False    0.000026\n",
      "27           tanks        O  False    0.000053\n",
      "28             and        O  False    0.000022\n",
      "29          troops        O  False    0.000148\n",
      "30            have        O  False    0.000022\n",
      "31         arrived        O  False    0.000211\n",
      "32              in        O  False    0.000328\n",
      "33             the        O  False    0.032308\n",
      "34        northern        O  False    0.024247\n",
      "35            city        O  False    0.009010\n",
      "36           Mosul    B-geo   True    0.848363\n",
      "37             for        O  False    0.000630\n",
      "38               a        O  False    0.000059\n",
      "39             new        O  False    0.019801\n",
      "40       offensive        O  False    0.000901\n",
      "41         against        O  False    0.000216\n",
      "42              al    B-org  False    0.031281\n",
      "43           Qaida    I-org  False    0.003638\n",
      "44              in    I-org  False    0.000529\n",
      "45            Iraq    I-org  False    0.976644\n",
      "46        fighters        O  False    0.013334\n",
      "47               .        O  False    0.000074\n",
      "48       Officials        O  False    0.000027\n"
     ]
    }
   ],
   "source": [
    "idx = slice(501, 550)\n",
    "print(pd.concat([ner.iloc[idx, :3].reset_index(drop=True),\n",
    "                 pd.DataFrame(model.predict(np.expand_dims(ner.iloc[idx, 3:].values, 0))[0][:, 1:],\n",
    "                              columns=['prob_geloc'])],\n",
    "                axis='columns'))\n",
    "\n",
    "# Go to https://geo-extract-tester.herokuapp.com/ and download\n",
    "# the validation data (validation.txt).\n",
    "with open('validation.txt', encoding='utf-8') as f:\n",
    "    s = f.read()\n",
    "\n",
    "gloved_data = glove.loc[[w for w in s.split('\\n') if w]].fillna(0)\n",
    "glove_time_size=100\n",
    "preds_batched = []\n",
    "i = 0\n",
    "while gloved_data[i:i+glove_time_size].size:\n",
    "    preds_batched.append(model.predict(np.expand_dims(gloved_data[i:i+glove_time_size], axis=0))[0][:,1])\n",
    "    i += glove_time_size\n",
    "\n",
    "with open('guesses.txt', 'w') as f:\n",
    "    for prob in [p for pred in preds_batched for p in pred]:\n",
    "        f.write(str(prob) + '\\n')\n",
    "\n",
    "# Now go to https://geo-extract-tester.herokuapp.com/ and upload `guesses.txt` to see how you did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
