{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin.rose\\Documents\\GitHub\\cjp-article-tagging\\lib\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tagnews\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n",
      "C:\\Users\\kevin.rose\\AppData\\Local\\Continuum\\Anaconda2\\envs\\cjp\\lib\\site-packages\\numpy\\lib\\arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "glove = tagnews.load_glove('tagnews/data/glove.6B.50d.txt')\n",
    "ner = tagnews.load_ner_data('tagnews/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('notebooks/training.txt', encoding='utf-8') as f:\n",
    "    training_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame([x.split() for x in training_data.split('\\n')], columns=['word', 'tag'])\n",
    "training_df.iloc[:,1] = training_df.iloc[:,1].apply(int)\n",
    "training_df['all_tags'] = 'NA'\n",
    "\n",
    "ner = training_df#pd.concat([training_df, ner]).reset_index(drop=True)\n",
    "ner = ner[['word', 'all_tags', 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ner = pd.concat([ner,\n",
    "                 pd.DataFrame(ner['word'].str[0].str.isupper().values),\n",
    "                 pd.DataFrame(glove.loc[ner['word'].str.lower()].values)],\n",
    "                axis='columns')\n",
    "ner.fillna(value=0.0, inplace=True)\n",
    "\n",
    "data_dim = 51\n",
    "timesteps = 25 # only during training, testing can take arbitrary length.\n",
    "num_classes = 2\n",
    "\n",
    "train_val_split = int(19 * ner.shape[0] / 20.)\n",
    "\n",
    "ner_train_idxs = range(0, train_val_split - timesteps, timesteps)\n",
    "x_train = np.array([ner.iloc[i:i+timesteps, 3:].values\n",
    "                    for i in ner_train_idxs])\n",
    "y_train = np.array([to_categorical(ner.iloc[i:i+timesteps, 2].values, 2)\n",
    "                    for i in ner_train_idxs])\n",
    "\n",
    "ner_val_idxs = range(train_val_split, ner.shape[0] - timesteps, timesteps)\n",
    "x_val = np.array([ner.iloc[i:i+timesteps, 3:].values\n",
    "                  for i in ner_val_idxs])\n",
    "y_val = np.array([to_categorical(ner.iloc[i:i+timesteps, 2].values, 2)\n",
    "                  for i in ner_val_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                                 Output Shape                            Param #        \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                                (None, None, 32)                        10752          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                                (None, None, 8)                         1312           \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistributed)         (None, None, 2)                         18             \n",
      "====================================================================================================\n",
      "Total params: 12,082\n",
      "Trainable params: 12,082\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(None, data_dim)))\n",
    "model.add(LSTM(8, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(2, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(model.summary(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='./tmp/weights-{epoch:02d}.hdf5',\n",
    "                               monitor='val_categorical_accuracy',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "class OurAUC(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Go to https://geo-extract-tester.herokuapp.com/ and download\n",
    "        # the validation data (validation.txt).\n",
    "        with open('validation.txt', encoding='utf-8') as f:\n",
    "            s = f.read()\n",
    "\n",
    "        gloved_data = pd.concat([pd.DataFrame([[w[0].isupper()] for w in s.split('\\n') if w]),\n",
    "                                 glove.loc[[w for w in s.split('\\n') if w]].fillna(0).reset_index(drop=True)],\n",
    "                               axis='columns')\n",
    "        \n",
    "        glove_time_size = 100\n",
    "        preds_batched = []\n",
    "        i = 0\n",
    "        while gloved_data[i:i+glove_time_size].size:\n",
    "            preds_batched.append(model.predict(np.expand_dims(gloved_data[i:i+glove_time_size],\n",
    "                                                              axis=0))[0][:,1])\n",
    "            i += glove_time_size\n",
    "\n",
    "        with open('guesses-{epoch:02d}.txt'.format(epoch=epoch), 'w') as f:\n",
    "            for prob in [p for pred in preds_batched for p in pred]:\n",
    "                f.write(str(prob) + '\\n')\n",
    "\n",
    "        with open('guesses-{epoch:02d}.txt'.format(epoch=epoch), 'rb') as f:\n",
    "            url = 'https://geo-extract-tester.herokuapp.com/api/score'\n",
    "            r = requests.post(url, files={'file': f})\n",
    "            print('AUC: {:.5f}'.format(json.loads(r.text)['auc']))\n",
    "\n",
    "our_auc = OurAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2467 samples, validate on 129 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.92992, saving model to ./tmp/weights-01.hdf5\n",
      "AUC: 0.88242\n",
      " - 16s - loss: 0.3766 - categorical_accuracy: 0.8850 - val_loss: 0.2403 - val_categorical_accuracy: 0.9299\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.92992 to 0.92992, saving model to ./tmp/weights-02.hdf5\n",
      "AUC: 0.92473\n",
      " - 13s - loss: 0.2043 - categorical_accuracy: 0.9165 - val_loss: 0.1696 - val_categorical_accuracy: 0.9299\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_categorical_accuracy did not improve\n",
      "AUC: 0.93838\n",
      " - 13s - loss: 0.1567 - categorical_accuracy: 0.9391 - val_loss: 0.1575 - val_categorical_accuracy: 0.9253\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_categorical_accuracy did not improve\n",
      "AUC: 0.94186\n",
      " - 12s - loss: 0.1458 - categorical_accuracy: 0.9423 - val_loss: 0.1501 - val_categorical_accuracy: 0.9290\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_categorical_accuracy did not improve\n",
      "AUC: 0.94330\n",
      " - 10s - loss: 0.1395 - categorical_accuracy: 0.9450 - val_loss: 0.1519 - val_categorical_accuracy: 0.9281\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.92992 to 0.93054, saving model to ./tmp/weights-06.hdf5\n",
      "AUC: 0.94593\n",
      " - 12s - loss: 0.1361 - categorical_accuracy: 0.9466 - val_loss: 0.1450 - val_categorical_accuracy: 0.9305\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_categorical_accuracy did not improve\n",
      "AUC: 0.94646\n",
      " - 11s - loss: 0.1328 - categorical_accuracy: 0.9473 - val_loss: 0.1472 - val_categorical_accuracy: 0.9296\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_categorical_accuracy did not improve\n",
      "AUC: 0.94771\n",
      " - 15s - loss: 0.1318 - categorical_accuracy: 0.9482 - val_loss: 0.1439 - val_categorical_accuracy: 0.9299\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.93054 to 0.93116, saving model to ./tmp/weights-09.hdf5\n",
      "AUC: 0.94737\n",
      " - 11s - loss: 0.1287 - categorical_accuracy: 0.9489 - val_loss: 0.1434 - val_categorical_accuracy: 0.9312\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_categorical_accuracy did not improve\n",
      "AUC: 0.94898\n",
      " - 13s - loss: 0.1276 - categorical_accuracy: 0.9491 - val_loss: 0.1462 - val_categorical_accuracy: 0.9287\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_categorical_accuracy did not improve\n",
      "AUC: 0.94845\n",
      " - 14s - loss: 0.1261 - categorical_accuracy: 0.9498 - val_loss: 0.1418 - val_categorical_accuracy: 0.9305\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.93116 to 0.93178, saving model to ./tmp/weights-12.hdf5\n",
      "AUC: 0.94865\n",
      " - 13s - loss: 0.1243 - categorical_accuracy: 0.9505 - val_loss: 0.1413 - val_categorical_accuracy: 0.9318\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.93178 to 0.93240, saving model to ./tmp/weights-13.hdf5\n",
      "AUC: 0.95034\n",
      " - 13s - loss: 0.1228 - categorical_accuracy: 0.9509 - val_loss: 0.1402 - val_categorical_accuracy: 0.9324\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.93240 to 0.93240, saving model to ./tmp/weights-14.hdf5\n",
      "AUC: 0.95042\n",
      " - 14s - loss: 0.1214 - categorical_accuracy: 0.9514 - val_loss: 0.1383 - val_categorical_accuracy: 0.9324\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_categorical_accuracy improved from 0.93240 to 0.93302, saving model to ./tmp/weights-15.hdf5\n",
      "AUC: 0.95048\n",
      " - 12s - loss: 0.1202 - categorical_accuracy: 0.9518 - val_loss: 0.1367 - val_categorical_accuracy: 0.9330\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_categorical_accuracy did not improve\n",
      "AUC: 0.94777\n",
      " - 12s - loss: 0.1185 - categorical_accuracy: 0.9526 - val_loss: 0.1373 - val_categorical_accuracy: 0.9324\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.93302 to 0.93426, saving model to ./tmp/weights-17.hdf5\n",
      "AUC: 0.94843\n",
      " - 13s - loss: 0.1176 - categorical_accuracy: 0.9527 - val_loss: 0.1397 - val_categorical_accuracy: 0.9343\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_categorical_accuracy did not improve\n",
      "AUC: 0.94558\n",
      " - 12s - loss: 0.1159 - categorical_accuracy: 0.9528 - val_loss: 0.1409 - val_categorical_accuracy: 0.9318\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_categorical_accuracy did not improve\n",
      "AUC: 0.94593\n",
      " - 12s - loss: 0.1151 - categorical_accuracy: 0.9529 - val_loss: 0.1379 - val_categorical_accuracy: 0.9293\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_categorical_accuracy did not improve\n",
      "AUC: 0.94591\n",
      " - 13s - loss: 0.1128 - categorical_accuracy: 0.9545 - val_loss: 0.1390 - val_categorical_accuracy: 0.9315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21555c49128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[checkpointer, our_auc],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word all_tags  tag  prob_geloc\n",
      "0            to       NA    0    0.032528\n",
      "1             a       NA    0    0.002263\n",
      "2          pool       NA    0    0.001998\n",
      "3            of       NA    0    0.002051\n",
      "4        reward       NA    0    0.001419\n",
      "5         money       NA    0    0.000777\n",
      "6           for       NA    0    0.000653\n",
      "7   information       NA    0    0.000576\n",
      "8       leading       NA    0    0.000635\n",
      "9            to       NA    0    0.000730\n",
      "10          the       NA    0    0.000955\n",
      "11       gunman       NA    0    0.000850\n",
      "12            â€™       NA    0    0.001382\n",
      "13            s       NA    0    0.002142\n",
      "14       arrest       NA    0    0.001075\n",
      "15            .       NA    0    0.001074\n",
      "16          Ald       NA    0    0.003206\n",
      "17            .       NA    0    0.001322\n",
      "18         Will       NA    0    0.001027\n",
      "19        Burns       NA    0    0.001338\n",
      "20            ,       NA    0    0.001105\n",
      "21           in       NA    0    0.001440\n",
      "22        whose       NA    0    0.000920\n",
      "23          4th       NA    1    0.031470\n",
      "24         Ward       NA    1    0.017477\n",
      "25            A       NA    0    0.001841\n",
      "26          man       NA    0    0.000848\n",
      "27          was       NA    0    0.000799\n",
      "28         shot       NA    0    0.000776\n",
      "29           to       NA    0    0.000906\n",
      "30        death       NA    0    0.000976\n",
      "31          and       NA    0    0.001114\n",
      "32           at       NA    0    0.003083\n",
      "33        least       NA    0    0.003131\n",
      "34           13       NA    0    0.002790\n",
      "35        other       NA    0    0.001529\n",
      "36       people       NA    0    0.001974\n",
      "37            ,       NA    0    0.002242\n",
      "38    including       NA    0    0.001874\n",
      "39            a       NA    0    0.001124\n",
      "40  14-year-old       NA    0    0.001115\n",
      "41          boy       NA    0    0.000778\n",
      "42            ,       NA    0    0.001104\n",
      "43         were       NA    0    0.001311\n",
      "44      wounded       NA    0    0.001379\n",
      "45        after       NA    0    0.001620\n",
      "46    shootings       NA    0    0.006381\n",
      "47     Saturday       NA    0    0.009718\n",
      "48    afternoon       NA    0    0.005640\n"
     ]
    }
   ],
   "source": [
    "idx = slice(501, 550)\n",
    "print(pd.concat([ner.iloc[idx, :3].reset_index(drop=True),\n",
    "                 pd.DataFrame(model.predict(np.expand_dims(ner.iloc[idx, 3:].values, 0))[0][:, 1:],\n",
    "                              columns=['prob_geloc'])],\n",
    "                axis='columns'))\n",
    "\n",
    "# Go to https://geo-extract-tester.herokuapp.com/ and download\n",
    "# the validation data (validation.txt).\n",
    "with open('validation.txt', encoding='utf-8') as f:\n",
    "    s = f.read()\n",
    "    \n",
    "gloved_data = glove.loc[[w for w in s.split('\\n') if w]].fillna(0)\n",
    "gloved_data.insert(0, 'case', [w[0].isupper() for w in s.split('\\n') if w])\n",
    "glove_time_size=100\n",
    "preds_batched = []\n",
    "i = 0\n",
    "while gloved_data[i:i+glove_time_size].size:\n",
    "    preds_batched.append(model.predict(np.expand_dims(gloved_data[i:i+glove_time_size], axis=0))[0][:,1])\n",
    "    i += glove_time_size\n",
    "\n",
    "with open('guesses.txt', 'w') as f:\n",
    "    for prob in [p for pred in preds_batched for p in pred]:\n",
    "        f.write(str(prob) + '\\n')\n",
    "\n",
    "# Now go to https://geo-extract-tester.herokuapp.com/ and upload `guesses.txt` to see how you did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
