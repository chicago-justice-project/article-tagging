{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joshherzberg/Documents/chihack/article-tagging/lib\n"
     ]
    }
   ],
   "source": [
    "cd '/Users/joshherzberg/Documents/chihack/article-tagging/lib/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tagnews\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import keras\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = tagnews.load_glove('tagnews/data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.loc['address_vec'] = glove.loc[['street', 'avenue', 'place', 'road', 'block', 'main', 'city', 'west', 'east', 'north', 'south']].mean()\n",
    "glove.loc['neighborhood_vec'] = glove.loc[['neighborhood', 'burrough', 'community', 'area']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagnews/data/Chicago_Street_Names.csv') as street_names:\n",
    "    streets = street_names.read().splitlines()[1:]\n",
    "streets = [i.lower() for i in streets]\n",
    "\n",
    "with open('tagnews/data/chicago_neighborhoods.csv') as neighborhoods:\n",
    "    hoods = neighborhoods.read().splitlines()\n",
    "hoods = list(set([j.lower().replace('\\\"', '') for j in hoods]))\n",
    "\n",
    "for name in streets:\n",
    "    glove.loc[name] = glove.loc['address_vec']\n",
    "for hood in hoods:\n",
    "    glove.loc[hood] = glove.loc['neighborhood_vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tagnews/data/training.txt', encoding='utf-8') as f:\n",
    "    our_training_data = f.read()\n",
    "    \n",
    "training_df = pd.DataFrame([x.split() for x in our_training_data.split('\\n') if x],\n",
    "                           columns=['word', 'tag'])\n",
    "\n",
    "training_df.iloc[:,1] = training_df.iloc[:,1].apply(int)\n",
    "training_df['all_tags'] = 'NA'\n",
    "\n",
    "# If you want to join our data w/ kaggle data, you can do this.\n",
    "# ner = tagnews.load_ner_data('tagnews/data/')\n",
    "# pd.concat([training_df, ner]).reset_index(drop=True)\n",
    "\n",
    "# If you just want to use our data, you can do this.\n",
    "ner = training_df\n",
    "\n",
    "ner = ner[['word', 'all_tags', 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(glove.loc[ner.loc[ner['word'] == 'Woodlawn']['word'].str.lower()].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "ner = pd.concat([ner,\n",
    "                 pd.DataFrame(ner['word'].str[0].str.isupper().values),\n",
    "                 pd.DataFrame(glove.loc[ner['word'].str.lower()].values),\n",
    "                 pd.DataFrame(ner['word'].str.isnumeric().values),\n",
    "                 pd.DataFrame(ner['word'].str.len().values)],\n",
    "                 axis='columns')\n",
    "ner.fillna(value=0.0, inplace=True)\n",
    "\n",
    "data_dim = 53\n",
    "timesteps = 25 # only during training, testing can take arbitrary length.\n",
    "num_classes = 2\n",
    "\n",
    "train_val_split = int(19 * ner.shape[0] / 20.)\n",
    "\n",
    "ner_train_idxs = range(0, train_val_split - timesteps, timesteps)\n",
    "x_train = np.array([ner.iloc[i:i+timesteps, 3:].values\n",
    "                    for i in ner_train_idxs])\n",
    "y_train = np.array([to_categorical(ner.iloc[i:i+timesteps, 2].values, 2)\n",
    "                    for i in ner_train_idxs])\n",
    "\n",
    "ner_val_idxs = range(train_val_split, ner.shape[0] - timesteps, timesteps)\n",
    "x_val = np.array([ner.iloc[i:i+timesteps, 3:].values\n",
    "                  for i in ner_val_idxs])\n",
    "y_val = np.array([to_categorical(ner.iloc[i:i+timesteps, 2].values, 2)\n",
    "                  for i in ner_val_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                                 Output Shape                            Param #        \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                                (None, None, 32)                        11008          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                                (None, None, 8)                         1312           \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistributed)         (None, None, 2)                         18             \n",
      "====================================================================================================\n",
      "Total params: 12,338\n",
      "Trainable params: 12,338\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(None, data_dim)))\n",
    "model.add(LSTM(8, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(2, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "print(model.summary(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)\n",
    "checkpointer = ModelCheckpoint(filepath='./tmp/weights-{epoch:02d}.hdf5',\n",
    "                               monitor='val_categorical_accuracy',\n",
    "                               verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "class OurAUC(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Go to https://geo-extract-tester.herokuapp.com/ and download\n",
    "        # the validation data (validation.txt).\n",
    "        '''with open('validation.txt', encoding='utf-8') as f:\n",
    "            s = f.read()\n",
    "\n",
    "        gloved_data = pd.concat([pd.DataFrame([[w[0].isupper()] for w in s.split('\\n') if w]),\n",
    "                                 glove.loc[[w for w in s.split('\\n') if w]].fillna(0).reset_index(drop=True)],\n",
    "                                 axis='columns')\n",
    "        glove_time_size = 100\n",
    "        preds_batched = []\n",
    "        i = 0\n",
    "        while gloved_data[i:i+glove_time_size].size:\n",
    "            preds_batched.append(model.predict(np.expand_dims(gloved_data[i:i+glove_time_size],\n",
    "                                                              axis=0))[0][:,1])\n",
    "            i += glove_time_size\n",
    "\n",
    "        with open('guesses-{epoch:02d}.txt'.format(epoch=epoch), 'w') as f:\n",
    "            for prob in [p for pred in preds_batched for p in pred]:\n",
    "                f.write(str(prob) + '\\n')'''\n",
    "\n",
    "        with open('tagnews/data/validation.txt', encoding='utf-8') as f:\n",
    "            s = f.read()\n",
    "\n",
    "        gloved_data = pd.concat([pd.DataFrame([[w[0].isupper()] for w in s.split('\\n') if w]),\n",
    "                                 glove.loc[[w for w in s.split('\\n') if w]].fillna(0).reset_index(drop=True),\n",
    "                                 pd.DataFrame([[w[0].isnumeric()] for w in s.split('\\n') if w]),\n",
    "                                 pd.DataFrame([[len(w[0])] for w in s.split('\\n') if w])],\n",
    "                                 axis='columns')\n",
    "        glove_time_size = 100\n",
    "        preds_batched = []\n",
    "        i = 0\n",
    "        while gloved_data[i:i+glove_time_size].size:\n",
    "            preds_batched.append(model.predict(np.expand_dims(gloved_data[i:i+glove_time_size],\n",
    "                                                              axis=0))[0][:,1])\n",
    "            i += glove_time_size\n",
    "\n",
    "        with open('guesses-{epoch:02d}.txt'.format(epoch=epoch), 'w') as f:\n",
    "            for prob in [p for pred in preds_batched for p in pred]:\n",
    "                f.write(str(prob) + '\\n')\n",
    "\n",
    "        with open('guesses-{epoch:02d}.txt'.format(epoch=epoch), 'rb') as f:\n",
    "            url = 'https://geo-extract-tester.herokuapp.com/api/score'\n",
    "            r = requests.post(url, files={'file': f})\n",
    "            try:\n",
    "                print('AUC: {:.5f}'.format(json.loads(r.text)['auc']))\n",
    "            except KeyError:\n",
    "                raise ValueError('Problem retrieving AUC from API. Is your validation set up to date?')\n",
    "\n",
    "our_auc = OurAUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6961 samples, validate on 366 samples\n",
      "Epoch 1/20\n",
      " - 7s - loss: 0.0786 - categorical_accuracy: 0.9685 - val_loss: 0.0670 - val_categorical_accuracy: 0.9793\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.97934, saving model to ./tmp/weights-01.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.95681\n",
      "Epoch 2/20\n",
      " - 7s - loss: 0.0670 - categorical_accuracy: 0.9752 - val_loss: 0.0624 - val_categorical_accuracy: 0.9813\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.97934 to 0.98131, saving model to ./tmp/weights-02.hdf5\n",
      "AUC: 0.96140\n",
      "Epoch 3/20\n",
      " - 7s - loss: 0.0629 - categorical_accuracy: 0.9762 - val_loss: 0.0654 - val_categorical_accuracy: 0.9812\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.98131\n",
      "AUC: 0.96158\n",
      "Epoch 4/20\n",
      " - 7s - loss: 0.0609 - categorical_accuracy: 0.9769 - val_loss: 0.0605 - val_categorical_accuracy: 0.9820\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.98131 to 0.98197, saving model to ./tmp/weights-04.hdf5\n",
      "AUC: 0.96314\n",
      "Epoch 5/20\n",
      " - 7s - loss: 0.0593 - categorical_accuracy: 0.9773 - val_loss: 0.0581 - val_categorical_accuracy: 0.9816\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.98197\n",
      "AUC: 0.96532\n",
      "Epoch 6/20\n",
      " - 7s - loss: 0.0579 - categorical_accuracy: 0.9776 - val_loss: 0.0595 - val_categorical_accuracy: 0.9809\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.98197\n",
      "AUC: 0.96490\n",
      "Epoch 7/20\n",
      " - 13s - loss: 0.0568 - categorical_accuracy: 0.9781 - val_loss: 0.0569 - val_categorical_accuracy: 0.9820\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.98197\n",
      "AUC: 0.96577\n",
      "Epoch 8/20\n",
      " - 7s - loss: 0.0557 - categorical_accuracy: 0.9782 - val_loss: 0.0557 - val_categorical_accuracy: 0.9825\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.98197 to 0.98251, saving model to ./tmp/weights-08.hdf5\n",
      "AUC: 0.96486\n",
      "Epoch 9/20\n",
      " - 8s - loss: 0.0549 - categorical_accuracy: 0.9787 - val_loss: 0.0560 - val_categorical_accuracy: 0.9823\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.98251\n",
      "AUC: 0.96570\n",
      "Epoch 10/20\n",
      " - 8s - loss: 0.0540 - categorical_accuracy: 0.9787 - val_loss: 0.0547 - val_categorical_accuracy: 0.9826\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.98251 to 0.98262, saving model to ./tmp/weights-10.hdf5\n",
      "AUC: 0.96614\n",
      "Epoch 11/20\n",
      " - 7s - loss: 0.0537 - categorical_accuracy: 0.9790 - val_loss: 0.0522 - val_categorical_accuracy: 0.9825\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.98262\n",
      "AUC: 0.96580\n",
      "Epoch 12/20\n",
      " - 8s - loss: 0.0525 - categorical_accuracy: 0.9791 - val_loss: 0.0516 - val_categorical_accuracy: 0.9833\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.98262 to 0.98328, saving model to ./tmp/weights-12.hdf5\n",
      "AUC: 0.96524\n",
      "Epoch 13/20\n",
      " - 7s - loss: 0.0514 - categorical_accuracy: 0.9798 - val_loss: 0.0538 - val_categorical_accuracy: 0.9834\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.98328 to 0.98339, saving model to ./tmp/weights-13.hdf5\n",
      "AUC: 0.96361\n",
      "Epoch 14/20\n",
      " - 8s - loss: 0.0510 - categorical_accuracy: 0.9799 - val_loss: 0.0520 - val_categorical_accuracy: 0.9838\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.98339 to 0.98383, saving model to ./tmp/weights-14.hdf5\n",
      "AUC: 0.96518\n",
      "Epoch 15/20\n",
      " - 7s - loss: 0.0498 - categorical_accuracy: 0.9802 - val_loss: 0.0579 - val_categorical_accuracy: 0.9819\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.98383\n",
      "AUC: 0.96081\n",
      "Epoch 16/20\n",
      " - 8s - loss: 0.0493 - categorical_accuracy: 0.9805 - val_loss: 0.0519 - val_categorical_accuracy: 0.9838\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.98383\n",
      "AUC: 0.95905\n",
      "Epoch 17/20\n",
      " - 8s - loss: 0.0486 - categorical_accuracy: 0.9809 - val_loss: 0.0525 - val_categorical_accuracy: 0.9836\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.98383\n",
      "AUC: 0.96047\n",
      "Epoch 18/20\n",
      " - 8s - loss: 0.0479 - categorical_accuracy: 0.9812 - val_loss: 0.0499 - val_categorical_accuracy: 0.9834\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.98383\n",
      "AUC: 0.95594\n",
      "Epoch 19/20\n",
      " - 8s - loss: 0.0468 - categorical_accuracy: 0.9814 - val_loss: 0.0535 - val_categorical_accuracy: 0.9831\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.98383\n",
      "AUC: 0.95924\n",
      "Epoch 20/20\n",
      " - 7s - loss: 0.0459 - categorical_accuracy: 0.9819 - val_loss: 0.0506 - val_categorical_accuracy: 0.9835\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.98383\n",
      "AUC: 0.95606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a22f54320>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[checkpointer, our_auc],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
